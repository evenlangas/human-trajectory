{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4378d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c685880",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_csv('dataCompressed12_5_100.csv')\n",
    "df = pd.read_csv('Data_Latest.csv')\n",
    "#df = pd.read_csv('dataCompressed3_125_100.csv')\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc0c97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec26c618",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df.drop('series_id', axis=1)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb87421",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = list(df.columns)\n",
    "features.remove(\"target\")\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7642149",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "df[features] = scaler.fit_transform(df[features])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724ce0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "def create_sequences(data, seq_length):\n",
    "    xs, ys = [], []\n",
    "    for i in range(0, len(data) - seq_length, round(seq_length/2)):\n",
    "        x = data[i:i+seq_length][['x_0', 'x_1']].values\n",
    "        y0 = data.iloc[i]['target']\n",
    "        y = data.iloc[i+seq_length]['target']\n",
    "        #xs.append(x)\n",
    "        #ys.append(y)\n",
    "        if y0 == y:\n",
    "            xs.append(x)\n",
    "            ys.append(y)\n",
    "    return np.array(xs), np.array(ys)\n",
    "SEQ_LENGTH = 20\n",
    "#SEQ_LENGTH = 80\n",
    "x_data, y_data = create_sequences(df, SEQ_LENGTH)\n",
    "print(len(y_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65f845e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699760c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(x_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb9a3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder lists for the final training and test sets\n",
    "x_train_list, x_test_list = [], []\n",
    "y_train_list, y_test_list = [], []\n",
    "for label in df['target'].unique():\n",
    "    x_data_class, y_data_class = create_sequences(df[df['target'] == label], SEQ_LENGTH)\n",
    "    train_size = int(len(x_data_class) * 0.8)\n",
    "\n",
    "    # Split the data for this class\n",
    "    x_train_class, x_test_class = x_data_class[:train_size], x_data_class[train_size:]\n",
    "    y_train_class, y_test_class = y_data_class[:train_size], y_data_class[train_size:]\n",
    "\n",
    "    # Append to the final lists\n",
    "    x_train_list.append(x_train_class)\n",
    "    x_test_list.append(x_test_class)\n",
    "    y_train_list.append(y_train_class)\n",
    "    y_test_list.append(y_test_class)\n",
    "\n",
    "# Concatenate data from all classes to get the final training and test sets\n",
    "x_train = np.concatenate(x_train_list, axis=0)\n",
    "x_test = np.concatenate(x_test_list, axis=0)\n",
    "y_train = np.concatenate(y_train_list, axis=0)\n",
    "y_test = np.concatenate(y_test_list, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79425588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For training data\n",
    "unique_labels_train, counts_train = np.unique(y_train, return_counts=True)\n",
    "for label, count in zip(unique_labels_train, counts_train):\n",
    "    print(f\"Label {label} in training data: {count} instances\")\n",
    "\n",
    "print(\"\\n\")  # Just to separate the outputs\n",
    "\n",
    "# For test data\n",
    "unique_labels_test, counts_test = np.unique(y_test, return_counts=True)\n",
    "for label, count in zip(unique_labels_test, counts_test):\n",
    "    print(f\"Label {label} in test data: {count} instances\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5881338a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.metrics import SparseCategoricalAccuracy\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import LSTM, BatchNormalization, Dense, Flatten, Bidirectional\n",
    "\n",
    "metrics = [SparseCategoricalAccuracy(name=\"accuracy\")]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(300, activation='relu', return_sequences=True, input_shape=(SEQ_LENGTH, 2))))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Bidirectional(LSTM(200, activation='relu', return_sequences=True)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Bidirectional(LSTM(100, activation='relu', return_sequences=True)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Bidirectional(LSTM(50, activation='relu')))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Flatten())\n",
    "model.add(Dense(200, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde86c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install keras-tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1ce1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "import tensorflow as tf\n",
    "def get_available_devices():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos]\n",
    "\n",
    "print(get_available_devices())\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f8a28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_tuner import RandomSearch\n",
    "\n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(LSTM(\n",
    "        units=hp.Int('units_1', min_value=200, max_value=500, step=50),\n",
    "        activation='relu',\n",
    "        return_sequences=True,\n",
    "        input_shape=(SEQ_LENGTH, 2)\n",
    "    )))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Bidirectional(LSTM(\n",
    "        units=hp.Int('units_2', min_value=100, max_value=300, step=50),\n",
    "        activation='relu',\n",
    "        return_sequences=True\n",
    "    )))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Bidirectional(LSTM(\n",
    "        units=hp.Int('units_3', min_value=50, max_value=200, step=50),\n",
    "        activation='relu'\n",
    "    )))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(\n",
    "        units=hp.Int('dense_units', min_value=100, max_value=300, step=50),\n",
    "        activation='relu'\n",
    "    ))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=hp.Choice('optimizer', values=['adam', 'sgd', 'rmsprop']),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=metrics\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff49eeae",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=10,  # or however many trials you wish to run\n",
    "    #directory='C:\\\\Users\\\\evenf\\\\OneDrive - Universitetet i Agder\\\\Even and Hamza PhD project work\\\\Human trajectory simulations paper\\\\LSTM Model with Dataset\\\\keras_tuner_dir'\n",
    ")\n",
    "\n",
    "tuner.search(x_train, y_train, epochs=8, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24c5072",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import Callback, CSVLogger, EarlyStopping, LearningRateScheduler, ReduceLROnPlateau\n",
    "import tensorflow as tf\n",
    "csv_logger = CSVLogger('logs.csv', separator = ',', append = True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.01, patience=3, verbose=1)\n",
    "def custom_lr_schedule(epoch):\n",
    "    if epoch < 30:\n",
    "        return 0.001\n",
    "    else:\n",
    "        return 0.001 * tf.math.exp(0.1 * (30 - epoch))\n",
    "lr_scheduler = LearningRateScheduler(custom_lr_schedule, verbose=1)\n",
    "lstm_history = model.fit(x_train, y_train, epochs=100, validation_data=(x_test, y_test), batch_size=16, callbacks=[reduce_lr, lr_scheduler, csv_logger])\n",
    "\n",
    "model.save('model_bilstm.keras')\n",
    "np.save('history_bilstm.npy',lstm_history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d41864b",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(x_test)\n",
    "classes_x=np.argmax(predictions,axis=1)\n",
    "\n",
    "print(x_test[1].shape)\n",
    "print(predictions.shape)\n",
    "print(classes_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548b1bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = tf.math.confusion_matrix(labels=y_test, predictions=classes_x, num_classes=10)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2f3fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "font_style = {'family' : 'sans-serif', # 'Times New Roman'\n",
    "        'weight' : 'normal',\n",
    "        'size'   : 14}\n",
    "\n",
    "font_style_nr = {'family' : 'sans-serif',\n",
    "        'weight' : 'normal',\n",
    "        'size'   : 12}\n",
    "\n",
    "hm = sns.heatmap(cm, annot=True, cmap='Blues',fmt='g', annot_kws={'fontdict': font_style_nr})\n",
    "hm.set_xticklabels(hm.get_xticklabels(), fontdict=font_style_nr)\n",
    "hm.set_yticklabels(hm.get_yticklabels(), fontdict=font_style_nr)\n",
    "cbar = hm.collections[0].colorbar\n",
    "# Set font style for colorbar tick labels\n",
    "for label in cbar.ax.get_yticklabels():\n",
    "    label.set_fontsize(12)\n",
    "    label.set_fontname('sans-serif')\n",
    "    label.set_fontweight('normal')\n",
    "\n",
    "plt.xlabel('Predicted Labels', fontdict=font_style)\n",
    "plt.ylabel('True Labels', fontdict=font_style)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb060fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, classes_x))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
